<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
  <title>Home - «Analaysis»</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <link rel="stylesheet" href="../theme/css/main.css" type="text/css" />
  <link href="../feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Home - Flux ATOM" />
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  "HTML-CSS": {
  styles: {
  ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
  },
  tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']],processEscapes: true}
  });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

</head>
<body>
<div id="page">

  <header id="header">
    <h1><a href="../index.html">Home</a></h1>
  </header>

<nav id="menu">
  <a href="../index.html">Home</a>
  <a href="../categories.html">Categories</a>
  <a href="../tags.html">Most Visited</a>
  <a href="../archives.html">Archives</a>

</nav> <!-- /#nav -->
  <section id="content">
 <h2 class="page_title">Articles avec le mot-clé «Analaysis»</h2>
 
    <article class="post">
      <h2 class="title"><a href="../ftrl_adp.html">Follow the Regularized Leader with Adaptive Decaying Proximal</a></h2>
        <details class="meta">
          Publié le <time datetime="2017-05-15T22:10:00+02:00" pubdate="pubdate">Sen 15 Mei 2017</time>
 par Andy Huynh dans «<a href="../category/neural-network.html">Neural Network</a>».  
Mots-clés: <a href="../tag/linear-algebra.html">Linear Algebra</a>, <a href="../tag/analaysis.html">Analaysis</a></p>        </details> 
      <section class="post_content">
        
<div class="cell border-box-sizing text_cell rendered">

<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Derivation-of-FTRL-ADP">Derivation of FTRL-ADP<a class="anchor-link" href="#Derivation-of-FTRL-ADP">¶</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">

<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Problem">Problem<a class="anchor-link" href="#Problem">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">

<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation} \label{eq:ftrl_dp}
\begin{split}
w_{t+1} = \underset{w}{\operatorname{argmax}} &\bigg\{g^\top _{1:t}w + \lambda_1\|w\|_1 + \frac{1}{2}\lambda_2\|w\|_2^2 + \frac{1}{2}\lambda_p\sum_{s=1}^{t}\sigma_{t,s} \|w-w_s\|^2_2\bigg\} \\ &\text{ in which } g^\top _{1:t} = \sum_{i=1}^{t}g^\top _t \text{ and } \sigma_{t,s} = \gamma^{t-s}
\end{split}
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">

<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is a variant of the FTRL-proximal proposed by McMahan et al. in Ad click prediction: a view from the trenches.</p><br />
        <a class="more" href="../ftrl_adp.html">Lire la suite...</a>
      </section>
    </article>
 
    <article class="post">
      <h2 class="title"><a href="../back_propagation.html">How Backpropagation Work?</a></h2>
        <details class="meta">
          Publié le <time datetime="2017-01-14T22:10:00+01:00" pubdate="pubdate">Sab 14 Januari 2017</time>
 par Andy Huynh dans «<a href="../category/neural-network.html">Neural Network</a>».  
Mots-clés: <a href="../tag/linear-algebra.html">Linear Algebra</a>, <a href="../tag/analaysis.html">Analaysis</a></p>        </details> 
      <section class="post_content">
        
<div class="cell border-box-sizing text_cell rendered">

<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Introduction">Introduction<a class="anchor-link" href="#Introduction">¶</a></h1><h2 id="Forward-Operation-and-4-Backpropagation-Formulas">Forward Operation and 4 Backpropagation Formulas<a class="anchor-link" href="#Forward-Operation-and-4-Backpropagation-Formulas">¶</a></h2><p><img src="images/network.png" width="450" /></p>
<p>By definition, the <strong>forward operation</strong> can be fully defined by 3 formulas.</p>
$$z^l = w^l a^{l-1} + b^l$$$$a^l = \sigma(z^l)$$$$C = \frac{1}{2} \|y(x)-a^L(x)\|^2$$<p>Set $\delta_j^l = \frac{\partial C}{\partial z_j^l}$. The following 4 backpropagation formulas are the direct consequences of the previous 3 formulas.</p>
$$\delta^L = \Delta_a C \odot \sigma' (z^L)$$$$\delta^l = ((w^{l+1})^T \delta^{l+1} ) \odot \sigma' (z^l)$$$$\frac{\partial C}{\partial b_j^l} = \delta_j^l$$$$\frac{\partial C}{\partial w_{jk}^{l}} = a_k^{l-1} \delta_j^l$$<p>These 4 formulas can be used to quickly compute the derivatives of the cost function with respect to the weights and the biases. The computation of derivatives starts from the output layer backwards to the first hidden layer, hence the name <strong>backpropagation algorithm</strong><br />
        <a class="more" href="../back_propagation.html">Lire la suite...</a>
      </section>
    </article>
  </section> <!-- /#content -->

<aside id="sidebar">

  <div class="widget" id="categories">
    <h2>Categories</h2>
    <ul>
      <li ><a href="../category/computer-vision.html">Computer Vision</a></li>
      <li ><a href="../category/cool-ml-projects.html">Cool ML Projects</a></li>
      <li ><a href="../category/neural-network.html">Neural Network</a></li>
    </ul>
  </div>

  

    <div class="widget" id="social">
      <h2>Social</h2>
      <ul>
        <li><a href="https://github.com/experiencor">github</a></li>
        <li><a href="https://twitter.com/experiencor">twitter</a></li>
       </ul>
    </div>

</aside>

  <footer id="footer">
    <p>Propulsé par <a href="http://docs.notmyidea.org/alexis/pelican/index.html">Pelican</a>.</p>
  </footer>
</div> <!-- /#page -->
<script id="dsq-count-scr" src="//andyhuynh-1.disqus.com/count.js" async></script>
</body>
</html>